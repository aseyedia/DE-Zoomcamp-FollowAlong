# DE-Zoomcamp-FollowAlong

This repository documents my journey as I progress through the [Data Engineering Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp) by [DataTalks](https://datatalks.club/). The bootcamp is designed to introduce students to the fundamental concepts and tools utilized in the field of data engineering. By working through this course, I aim to gain hands-on experience with a variety of open-source data engineering tools, platforms, and best practices.

## Course Overview

The bootcamp spans 7 weeks, each focusing on a specific topic or set of tools in the domain of data engineering:

### Week 1: Introduction & Prerequisites

- Understanding of the course structure and goals.
- Getting started with Google Cloud Platform (GCP).
- Introduction to containerization with Docker and `docker-compose`.
- Setting up and running Postgres using Docker.
- Infrastructure provisioning on GCP using Terraform.
- Environment setup to facilitate subsequent weeks.

### Week 2: Workflow Orchestration

- Concepts behind a Data Lake.
- Principles of workflow orchestration.
- Deep dive into Prefect - a workflow management system.
- ETL operations using GCP & Prefect.
- Making workflows dynamic with parameterization.
- Introduction to Prefect Cloud and exploring its resources.

### Week 3: Data Warehouse

- Understanding a Data Warehouse.
- Introduction to Google BigQuery.
- Data optimization techniques like partitioning and clustering.
- Best practices for using BigQuery.
- Internals and workings of BigQuery.
- Integration of BigQuery with Apache Airflow.
- Machine learning capabilities within BigQuery.

### Week 4: Analytics Engineering

- Basics of analytics engineering.
- Introduction to dbt (data build tool).
- Working with dbt in BigQuery and Postgres.
- Creating dbt models.
- Ensuring data quality with testing and documentation.
- Deployment strategies - cloud and local.
- Data visualization using Google Data Studio and Metabase.

### Week 5: Batch Processing

- Introduction to batch processing paradigms.
- Overview of Apache Spark.
- Working with Spark DataFrames.
- SQL operations with Spark.
- In-depth understanding of GroupBy and join operations in Spark.

### Week 6: Streaming Processing

- Introduction to Apache Kafka.
- Working with schemas using Avro.
- Streaming data with Kafka Streams.
- Integration and operations with Kafka Connect and KSQL.

### Week 7: Capstone Project

- Application of concepts learned throughout the course.
- Real-world data engineering tasks and problem solving.

## Objectives

By following this bootcamp, I aim to:

- Develop proficiency in using prominent data engineering tools.
- Understand the architecture and principles behind data lakes, data warehouses, and data pipelines.
- Learn best practices for analytics engineering and data visualization.
- Familiarize myself with both batch and streaming data processing paradigms.
- Apply learned concepts in real-world scenarios via a comprehensive project.

## Tools & Technologies Used

Throughout this course, a myriad of tools and platforms will be explored, including:

- **Google Cloud Platform (GCP)**: A suite of cloud computing services and APIs.
- **Docker**: A platform for developing, shipping, and running applications inside containers.
- **Prefect**: A workflow management system.
- **Apache Airflow**: A platform to programmatically author, schedule, and monitor workflows.
- **Terraform**: An open-source infrastructure as code software tool.
- **Google BigQuery**: A serverless, highly scalable, and cost-effective multi-cloud data warehouse.
- **dbt**: A data build tool for transforming data in the warehouse.
- **Apache Spark**: An open-source distributed computing system.
- **Apache Kafka**: A distributed event streaming platform.
